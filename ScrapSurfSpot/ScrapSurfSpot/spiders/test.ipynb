{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# CODE fonctionel   ########################\n",
    "#scrapy shell 'https://www.surf-report.com/meteo-surf/cap-gris-nez-s1108.html'\n",
    "\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'SurfSpot'\n",
    "    allowed_domains = ['www.surf-report.com']\n",
    "    start_urls = ['https://www.surf-report.com/meteo-surf/france/',\n",
    "                  'https://www.surf-report.com/meteo-surf/france/?pageId=2'\n",
    "                 ]\n",
    " \n",
    "    def parse(self, response):\n",
    "        #Différents liens peremettant l'accés aux infos sur les spots \n",
    "        links = response.css(\".grid_1 .card-content .title\").css('a::attr(href)').extract()\n",
    "        i=0\n",
    "        #Création d'url propre pour les liens \n",
    "        for url in links[:10]:\n",
    "            links[i] = response.urljoin(url)\n",
    "            i=i+1\n",
    "\n",
    "        #Application de la fonction \"parse_meteo\" pour chaque liens     \n",
    "        for link in links[:5]:\n",
    "            yield response.follow(link, self.parse_meteo)\n",
    "            \n",
    "        \n",
    "    def parse_meteo(self, response):\n",
    "        #Extraction du nom du spot via la barre de recherche \n",
    "             \n",
    "        #a = response.css(\"#searchQuery\").extract()          \n",
    "        #B = a[0].split('\"')\n",
    "        #name_spot =[]\n",
    "        #name_spot = B[7].split(',')\n",
    "        name_spot=response.css(\".searchBarInputContainer\").css(\"input::attr(placeholder)\").extract()\n",
    "        days = response.css(\".forecast-tab\").css(\".title\").css(\"b::text\").extract()\n",
    "        day_0 = days[0]\n",
    "        day_1 = days[1]\n",
    "        day_2 = days[2]\n",
    "        day_3 = days[3]\n",
    "        day_4 = days[4]\n",
    "       \n",
    "    \n",
    "        #Récupération de toutes les tailles de swell et des intervalles entre les vaguez  \n",
    "        swell_and_interval = response.css(\".forecast-tab\").css(\".swell\").css(\"span::text\").extract()\n",
    "        \n",
    "        #Calcul des listes des tailles de swell à différentes heures (données pour 5 horaires différentes) et différents jours\n",
    "        swell_one_day = []\n",
    "        swell_by_day =[]\n",
    "        x=0\n",
    "        for j in range(1,6):\n",
    "            \n",
    "            x = x+10\n",
    "            for i in range(-10+x,0+x): \n",
    "                if i%2 == 0:\n",
    "                    swell_one_day.append(swell_and_interval[i])\n",
    "                    \n",
    "                    if i == 8 or i == 18 or i == 28 or i == 38 or i == 48 or i == 58 :\n",
    "                        swell_by_day.append(swell_one_day)\n",
    "                        swell_one_day=[]\n",
    "      \n",
    "    \n",
    "        #Calcul des listes des intervalles de swell à différentes heures (5 horaires différentes) et différents jours (4 jours)\n",
    "        interval_one_day = []\n",
    "        interval_by_day =[]\n",
    "        x=0\n",
    "        for j in range(1,6):\n",
    "            \n",
    "            x = x+10\n",
    "            for i in range(-10+x,0+x): \n",
    "                if i%2 != 0:\n",
    "                    interval_one_day.append(swell_and_interval[i])\n",
    "                    \n",
    "                    if i == 9 or i == 19 or i == 29 or i == 39 or i == 49 or i == 59 :\n",
    "                        interval_by_day.append(interval_one_day)\n",
    "                        interval_one_day=[]\n",
    "\n",
    "       #Récupération des températures de l'air par jour et heure \n",
    "        temp_air = response.css(\".forecast-tab\").css(\".micro\").css(\"span::text\").extract()\n",
    "        temp_air_one_day = []\n",
    "        temp_air_by_day =[]\n",
    "        x=0\n",
    "        for j in range(1,6):\n",
    "            \n",
    "            x = x+5\n",
    "            for i in range(-5+x,0+x): \n",
    "                temp_air_one_day.append(temp_air[i])\n",
    "\n",
    "                if i == 4 or i == 9 or i == 14 or i == 19 or i == 24 or i == 39 :\n",
    "                    temp_air_by_day.append(temp_air_one_day)\n",
    "                    temp_air_one_day=[]\n",
    "      \n",
    "        #Récupération vitesse du vent par jour et heure \n",
    "        vitesse_vent = response.css(\".forecast-tab\").css(\".wind\").css(\"span::text\").extract()\n",
    "        vitesse_vent_one_day = []\n",
    "        vitesse_vent_by_day =[]\n",
    "        x=0\n",
    "        for j in range(1,6):\n",
    "            \n",
    "            x = x+5\n",
    "            for i in range(-5+x,0+x): \n",
    "                vitesse_vent_one_day.append(vitesse_vent[i])\n",
    "\n",
    "                if i == 4 or i == 9 or i == 14 or i == 19 or i == 24 or i == 39 :\n",
    "                    vitesse_vent_by_day.append(vitesse_vent_one_day)\n",
    "                    vitesse_vent_one_day=[]\n",
    "        \n",
    "        \n",
    "        #Calcule du temps qu'il fait grâce aux logos              \n",
    "        weather = response.css(\".forecast-tab\").css(\".weather\").css(\"img::attr(alt)\").extract()\n",
    "        weather_one_day = []\n",
    "        weather_by_day =[]\n",
    "        x=0\n",
    "        for j in range(1,6):\n",
    "            \n",
    "            x = x+5\n",
    "            for i in range(-5+x,0+x): \n",
    "                weather_one_day.append(weather[i])\n",
    "\n",
    "                if i == 4 or i == 9 or i == 14 or i == 19 or i == 24 or i == 39 :\n",
    "                    weather_by_day.append(weather_one_day)\n",
    "                    weather_one_day=[]\n",
    "              \n",
    " #['Ciel couvert',\n",
    " #'Ciel peu nuageux',\n",
    " #'Ciel nuageux',\n",
    " #'Averses de grêle',\n",
    " #'Ciel très nuageux',\n",
    " #'Pluie et neige faible',\n",
    " #'Averses de pluie',\n",
    " #'Voile de nuages élevés',\n",
    " #'Averses de grésil',\n",
    " #'Ciel clair',\n",
    " #'Pluie faible']\n",
    "        \n",
    "        #Orientation du swell             \n",
    "        orientation_swell = response.css(\".forecast-tab\").css(\".swell\").css(\"img::attr(alt)\").extract()\n",
    "        orientation_swell_one_day = []\n",
    "        orientation_swell_by_day =[]\n",
    "        x=0\n",
    "        for j in range(1,6):\n",
    "            \n",
    "            x = x+5\n",
    "            for i in range(-5+x,0+x): \n",
    "                orientation_swell_one_day.append(orientation_swell[i])\n",
    "\n",
    "                if i == 4 or i == 9 or i == 14 or i == 19 or i == 24 or i == 39 :\n",
    "                    orientation_swell_by_day.append(orientation_swell_one_day)\n",
    "                    orientation_swell_one_day=[]\n",
    "                    \n",
    "        #Orientation du vent             \n",
    "        orientation_wind = response.css(\".forecast-tab\").css(\".wind\").css(\"img::attr(alt)\").extract()\n",
    "        orientation_wind_one_day = []\n",
    "        orientation_wind_by_day =[]\n",
    "        \n",
    "        x=0\n",
    "        for j in range(1,6):\n",
    "            x = x+5\n",
    "            for i in range(-5+x,0+x): \n",
    "                orientation_wind_one_day.append(orientation_wind[i])\n",
    "\n",
    "                if i == 4 or i == 9 or i == 14 or i == 19 or i == 24 or i == 39 :\n",
    "                    orientation_wind_by_day.append(orientation_wind_one_day)\n",
    "                    orientation_wind_one_day=[]\n",
    "                                  \n",
    "        \n",
    "        yield ArticleItem(\n",
    "                name_spot : name_spot,\n",
    "                temp_air_by_day : temp_air_by_day, \n",
    "                interval_by_day : interval_by_day,\n",
    "                swell_by_day : swell_by_day,\n",
    "                vitesse_vent_by_day : vitesse_vent_by_day,\n",
    "                weather_by_day : weather_by_day,\n",
    "                orientation_wind_by_day : orientation_wind_by_day,\n",
    "                orientation_swell_by_day : orientation_swell_by_day,\n",
    "                day_0 : day_0,\n",
    "                day_1 : day_1,\n",
    "                day_2 : day_2,\n",
    "                day_3 : day_3,\n",
    "                day_4 : day_4,\n",
    "            \n",
    "            )           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load /Users/antoine/Desktop/Data engeneering/DataEngineerTools/2Scrapy/ScrapSurfSpot/ScrapSurfSpot/spiders/example.py\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'SurfSpot'\n",
    "    allowed_domains = ['www.surf-report.com']\n",
    "    start_urls = ['https://www.surf-report.com/meteo-surf/france/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "#        all_links = {\n",
    "#            response.urljoin(url) for name, url in zip(\n",
    "#           response.css(\".title\").css(\"a::text\").extract() \n",
    "#            response.css('a::attr(href)').extract()\n",
    "#        }\n",
    "        yield {\n",
    "            \"title\":title,\n",
    "#            \"all_links\":all_links\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 0: cd: ScrapSurfSpot: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cd ScrapSurfSpot && scrapy crawl example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'SurfSpot'\n",
    "    allowed_domains = ['www.surf-report.com']\n",
    "    start_urls = ['https://www.surf-report.com/meteo-surf/france/']\n",
    "    \n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = response.css(\".title\").css(\"b ::text\").extract()\n",
    "        all_links = {\n",
    "            response.urljoin(url) for name, url in zip(\n",
    "            response.css(\".title\").css(\"b ::text\").extract(), \n",
    "            response.css(\".card-content .title\").css('a::attr(href)').extract()[1:122]) \n",
    "        }\n",
    "        \n",
    "        yield {\n",
    "            \"title\":name,\n",
    "            \"all_links\":all_links\n",
    "        }\n",
    "\n",
    "        \n",
    "    def parse_category(self, response):\n",
    "        for spot in response.css(\".card-content .title\")[1:122]:\n",
    "            name = spot.css(\"b::text\").extract() \n",
    "            link = spot.css('a::attr(href)').extract()\n",
    "            #print(f\"Name {name} \\nLink {link}\\n ----\")\n",
    "            yield {\n",
    "                \"name\":title,\n",
    "                \"link\":link\n",
    "            }        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "V4\n",
    "#from ..items import ArticleItem\n",
    "\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'SurfSpot'\n",
    "    allowed_domains = ['www.surf-report.com']\n",
    "    start_urls = ['https://www.surf-report.com/meteo-surf/france/']\n",
    "    \n",
    "    \n",
    "    def parse(self, response):\n",
    "        title = \"titre\"\n",
    "        all_links = {\n",
    "            nom:response.urljoin(url) for nom, url in zip(\n",
    "            response.css(\".grid_1 .card-content .title\").css(\"b\").extract()[0:121], \n",
    "            response.css(\".grid_1 .card-content .title\").css('a::attr(href)').extract()[1:122]) \n",
    "        }\n",
    "        for link in all_links.values():\n",
    "            yield Request(link, callback=self.parse_category)\n",
    "\n",
    "        \n",
    "    def parse_category(self, response):\n",
    "        for spot in response.css(\".grid_1 .card-content .title\")[0:122]:\n",
    "            names = spot.css(\"b::text\").extract_first() \n",
    "            link = spot.css('a::attr(href)').extract_first()\n",
    "        #print(f\"Names {names} \\nLink {link}\\n ----\")\n",
    "            yield {\n",
    "                \"names\":names,\n",
    "                \"link\":link\n",
    "            }        \n",
    "            \n",
    "            \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrapy shell 'https://www.surf-report.com/meteo-surf/cap-gris-nez-s1108.html'\n",
    "\n",
    "\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "\n",
    "class ExampleSpider(scrapy.Spider):\n",
    "    name = 'SurfSpot'\n",
    "    allowed_domains = ['www.surf-report.com']\n",
    "    start_urls = ['https://www.surf-report.com/meteo-surf/france/',\n",
    "                 ]\n",
    "                \n",
    "                 \n",
    "    def parse(self, response):\n",
    "        #Follow links\n",
    "        link = response.css(\".grid_1 .card-content .title\").css('a::attr(href)').extract()\n",
    "        i=0\n",
    "        for url in link:\n",
    "            link[i] = response.urljoin(url)\n",
    "            i=i+1\n",
    "\n",
    "#        for href in response.css(\".pagination\").css('a::attr(href)').extract():\n",
    "#            yield response.follow(href, self.parse_link)\n",
    "            \n",
    "        for href2 in link[:10]:\n",
    "            yield response.follow(href2, self.parse_meteo)\n",
    "            \n",
    "           \n",
    "        \n",
    "        \n",
    "    def parse_meteo(self, response):\n",
    "                \n",
    "        meteo = response.css(\".forecast-live .title\").css(\"b::text\").extract()\n",
    "        air = response.css(\".forecast\").css(\".air::text\").extract()\n",
    "\n",
    "        for item2 in zip(meteo,air):\n",
    "            #create a dictionary to store the scraped info\n",
    "            scraped_info2 = {\n",
    "                'spot' : item2[0],\n",
    "                'air' : item2[1], \n",
    "            }\n",
    "\n",
    "            #yield or give the scraped info to scrapy\n",
    "            yield scraped_info2\n",
    "        \n",
    "                \n",
    "#    def parse_link(self, response):\n",
    "                \n",
    "#        titles = response.css(\".grid_1 .card-content .title\").css(\"b::text\").extract()\n",
    "#        link = response.css(\".grid_1 .card-content .title\").css('a::attr(href)').extract()\n",
    "#        i=0\n",
    "\n",
    "#        for url in link:\n",
    "#            link[i] = response.urljoin(url)\n",
    "#            i=i+1\n",
    "\n",
    "        #Give the extracted content row wise\n",
    "#        for item in zip(titles,link):\n",
    "            #create a dictionary to store the scraped info\n",
    "#            scraped_info = {\n",
    "#                'titles' : item[0],\n",
    "#                'link' : item[1],\n",
    "#            }\n",
    "\n",
    "            #yield or give the scraped info to scrapy\n",
    "#            yield scraped_info\n",
    "\n",
    "           \n",
    "             "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
